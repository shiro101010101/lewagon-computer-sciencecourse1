{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA : Face Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, we will use PCA in combination with a dataset of multiple black & white image**s**\n",
    "\n",
    "This time then:\n",
    "- each image is an observation (sample)\n",
    "- each pixel's luminosity level is a feature (lots of features!!)\n",
    "\n",
    "We will use PCA to compress the picture by reducing the number of feature used to describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a **lower-dimensional projection** of the data that preserves the maximal data variance.\n",
    "\n",
    "We can then use those new representations as features to feed any model we want. It can be very useful since you often have lots of features, and you want to transform and keep a packed number of features that are the most representative of what you want to model.\n",
    "\n",
    "__Lets load a face image dataset and apply PCA.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have access to\n",
    "- `faces.images` which are the matrix of __50 x 37 pixels__ you can plot \n",
    "- `faces.data` for the the flattened version in  of size __1850 x 1__ (because 50 x 37 = 1850). \n",
    "- `faces.target` which is annotation of every face to the corresponding person (as a number index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1288 images\n",
    "faces.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1288 images flattened, equivalent to to faces.images.reshape(1288,1850)\n",
    "faces.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s visualize some faces.\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(7,10))\n",
    "for i in range(15):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.title(faces.target_names[faces.target[i]], size=12)\n",
    "    plt.imshow(faces.images[i], cmap=plt.cm.gray)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress images with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scikit dataset we have 50 × 37 pixel images (1850 dimensions!). Often, so many dimensions is a lot to train algorithms we studied in the previous exercises (for example SVM). \n",
    "\n",
    "That's why we use PCA to reduce these features to a more manageable size, while maintaining most of the information of the dataset.\n",
    "\n",
    "__👉 Apply PCA to the dataset (both fit and transform), to reduce dimensions to 150, by setting `n_components=150`__. Put your transformation into a variable named `data_projected`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The face dataset was projected onto only the first 150 principal components! Again, what we call components are directions of most variance of the dataset. \n",
    "\n",
    "It means that now, we don't need 1850 pixels anymore to describe each images but just 150 values. A gain by factor 12.\n",
    "\n",
    "__How is that possible?__\n",
    "\n",
    "- The pca has found to be the most representative directions of what distinguishes faces between each other with just 150 values for every image. \n",
    "\n",
    "- They are the directions of most variance. \n",
    "\n",
    "- You can access them in `pca.components_`\n",
    "\n",
    "👉  Look at the first component of this array of components, and its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's a vector of 1850 values. We have now 150 components of 1850 values each.\n",
    "\n",
    "One face is now described as a combination (sum) of those components.\n",
    "\n",
    "Let's reconstruct one image from its reduced representation to see how it works.\n",
    "\n",
    "👉 Use `inverse_transform` on your `data_projected` to reconstruct a `data_reconstructed` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Plot the 13th picture (Gearge W Bush) of the reconstructed dataset, and compare it with the original one. \n",
    "\n",
    "<details>\n",
    "    <summary>💡Hint</summary>\n",
    "\n",
    "You'll have to reshape the flattened data into an \"image\" of dimension 2\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('projection', shape=data_projected.shape)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 (Optional) study the code below which reconstruct the image step by step without `inverse_transform`. \n",
    "\n",
    "Instead, we do manually the multiplication $X_{reconstructed} = X_{projected} W$ with $W$ being the `pca.components_` matrix of 150 PC in rows expressed as linear combination of 1850 pixels in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dimensions = 150\n",
    "\n",
    "# We do our reconstruction over the 13th image\n",
    "image_original = faces.images[12];\n",
    "image_compressed = data_projected[12];\n",
    "\n",
    "# we start the reconstruction from the mean over all images (feel free to print mean face to check!)\n",
    "image_reconstructed = pca.mean_.copy(); \n",
    "\n",
    "# Then, reconstruct the image by doing the sum of every 150 entry of its compressed representation,\n",
    "# weighted by the corresponding principal components\n",
    "for i in range(num_dimensions):\n",
    "    image_reconstructed += pca.components_[i] * image_compressed[i]\n",
    "    \n",
    "# Plot the original and the compressed image.\n",
    "fig, ax = plt.subplots(1, 2, figsize = (5,5))\n",
    "ax[0].imshow(image_original, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(image_reconstructed.reshape(faces.images[0].shape), cmap=plt.cm.gray)\n",
    "ax[1].set_title('Compressed reconstructed Image')\n",
    "for ax in fig.axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate your Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Image-plot the \"mean\" face of the dataset\n",
    "\n",
    "<details>\n",
    "    <summary>💡Hint</summary>\n",
    "\n",
    "\n",
    "You can use `pca.mean_` or `faces.data.mean(axis=0)`\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Access your first PC. What's its shape? Print it as pd.Series or NDarray. What does each values represents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each PC is a flatten \"image\" of 1850 pixels\n",
    "\n",
    "- Your first PC are the most important \"directions\" on of your 1850-dimension dataset.\n",
    "\n",
    "- They are the most important \"linear combination of your 1850 pixels\".\n",
    "\n",
    "- The ones which preserves the most \"variance\" when your dataset of pictures is projected onto it.\n",
    "\n",
    "- The first few PCs are the regions of the 2D pixel grid that bear the most differences between your 1288 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Image-Plot the **5 first** principal components, as well as the **last** one.\n",
    "Do you see more intuitively what PC are? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every image can be represented by the \"mean face\" plus a linear combination of the 150 \"PC faces\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose the Number of Components?\n",
    "\n",
    "In practice, it is very important to find how many components are needed to describe the data without losing too much information. This can be determined visually by plotting the cumulative sum of `explained_variance_ratio_` as a function of the number of components.\n",
    " \n",
    "👉 Plot it below for the first 150 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total variance is contained within the first components. For example:\n",
    "- The first 20 components contain more than 75% of the variance,\n",
    "- while we need about only 70 components to describe 90% of the variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❓ What is the minimal number of components you need to keep to get _at least_ 80% of the variance?  Assign the value to a variable called `minimal_pc_count`**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('components', min_pc = minimal_pc_count)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify images (PCA as feature engineering)\n",
    "\n",
    "You have this dataset of faces and you want to build a face recognition engine to predict `faces.target` which is annotation of every face to the corresponding person (as a number index)\n",
    "\n",
    "You can now use this low-dimensional new transformation you just created, that is still representative of the faces to train your supervised algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split the face dataset\n",
    "\n",
    "👉 Use the train test split function from scikit to separate __the original faces dataset__ into training and testing set, `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform your training set to reduce the number of dimensions / features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Fit a PCA __over the training data only__ and transform your training data into the reduced dimension (150 features for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Using this same PCA (only trained on the training set) transform your testing set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validate your choice of best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Call a cross validated grid search for an SVM, where you loop over all combinations of\n",
    "\n",
    "- kernels\n",
    "- \"C between 10 and 10000\n",
    "- \"gamma\" between 0.0001 and 0.1\n",
    "\n",
    "👉 Use `n_jobs=-1` to make use of all your core CPUs\n",
    "\n",
    "👉 Use `f1_weighted` as the performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>🔎 Hint </summary>\n",
    "    Remember the task at hand calls for classification, have a look at the <a src=https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC>Support Vector Classification </a> class.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Train that cross validation grid search over our newly transformed training set. (Long computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Print the [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) of your best model over the testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Try to improve this score with the best choice of PCA components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>💡 Hint</summary>\n",
    "    \n",
    "- Try to pipeline your preprocessing (PCA) and your modelling (SVC)\n",
    "- A pipeline is an instance of Scikit-Learn `Estimator` which hyperparameters can be grid searched\n",
    "- Consider `n_components` as an hyperparameter of your pipeline!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❓ How many components give the best score?  Assign the value to a variable called `best_n_components`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('classification', best_pc=best_n_components)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 **Congratulation! Don't forget to commit and push your notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow & Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Import the house price data set. We will only keep numerical feature for sake of simplicity\n",
    "\n",
    "Your goal will be to fit the best KNN Regressor. And in particular, how many \"neighbors\" (K in KNN) should you consider to best predict your house-price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830</td>\n",
       "      <td>...</td>\n",
       "      <td>736</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n",
       "0        1          60         65.0     8450            7            5   \n",
       "1        2          20         80.0     9600            6            8   \n",
       "2        3          60         68.0    11250            7            5   \n",
       "3        4          70         60.0     9550            7            5   \n",
       "4        5          60         84.0    14260            8            5   \n",
       "...    ...         ...          ...      ...          ...          ...   \n",
       "1455  1456          60         62.0     7917            6            5   \n",
       "1456  1457          20         85.0    13175            6            6   \n",
       "1457  1458          70         66.0     9042            7            9   \n",
       "1458  1459          20         68.0     9717            5            6   \n",
       "1459  1460          20         75.0     9937            5            6   \n",
       "\n",
       "      YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  WoodDeckSF  \\\n",
       "0          2003          2003       196.0         706  ...           0   \n",
       "1          1976          1976         0.0         978  ...         298   \n",
       "2          2001          2002       162.0         486  ...           0   \n",
       "3          1915          1970         0.0         216  ...           0   \n",
       "4          2000          2000       350.0         655  ...         192   \n",
       "...         ...           ...         ...         ...  ...         ...   \n",
       "1455       1999          2000         0.0           0  ...           0   \n",
       "1456       1978          1988       119.0         790  ...         349   \n",
       "1457       1941          2006         0.0         275  ...           0   \n",
       "1458       1950          1996         0.0          49  ...         366   \n",
       "1459       1965          1965         0.0         830  ...         736   \n",
       "\n",
       "      OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
       "0              61              0          0            0         0        0   \n",
       "1               0              0          0            0         0        0   \n",
       "2              42              0          0            0         0        0   \n",
       "3              35            272          0            0         0        0   \n",
       "4              84              0          0            0         0        0   \n",
       "...           ...            ...        ...          ...       ...      ...   \n",
       "1455           40              0          0            0         0        0   \n",
       "1456            0              0          0            0         0        0   \n",
       "1457           60              0          0            0         0     2500   \n",
       "1458            0            112          0            0         0        0   \n",
       "1459           68              0          0            0         0        0   \n",
       "\n",
       "      MoSold  YrSold  SalePrice  \n",
       "0          2    2008     208500  \n",
       "1          5    2007     181500  \n",
       "2          9    2008     223500  \n",
       "3          2    2006     140000  \n",
       "4         12    2008     250000  \n",
       "...      ...     ...        ...  \n",
       "1455       8    2007     175000  \n",
       "1456       2    2010     210000  \n",
       "1457       5    2010     266500  \n",
       "1458       4    2010     142125  \n",
       "1459       6    2008     147500  \n",
       "\n",
       "[1121 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data\n",
    "data = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv')\n",
    "\n",
    "# Only keep numerical columns and raws without NaN\n",
    "data = data.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['SalePrice'])\n",
    "y = data['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Split the data to create your `X_train` `X_test` and `y_train` `y_test`. Use:\n",
    "- `test_size=0.3`\n",
    "- `random_state=0` to compare with your buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is always very important for KNN.\n",
    "\n",
    "❓ _Standard-Scale_ your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ 5-fold cross validate a simple KNN regressor taking into account only the closest neighbor, and compute its mean cv-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.569025195507008"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "cross_validate(knn, X_train_scaled, y_train, cv=5)[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid search\n",
    "\n",
    "Let's use sklearn `GridSearchCV` to find the best KNN hyperparameter `n_neighbors`.\n",
    "- Start coarse-grain approach, with `n_neighbors` = [1,5,10,20,50]\n",
    "- 5-fold cross validate each combination\n",
    "- Be sure to maximize your performance time using `n_jobs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={'n_neighbors': [1, 5, 10, 20, 50]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Instanciate model\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "k_grid = {'n_neighbors' : [1,5,10,20,50]}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "grid = GridSearchCV(model, k_grid, n_jobs=-1,  cv = 5)\n",
    "\n",
    "# Fit data to Grid Search\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ According to the grid search, what is the optimal K value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 20}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What is the best score the optimal K value produced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7594265833471792"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an idea about where the best k lies, but some of the values we did not try could be better!\n",
    "\n",
    "Re-run a fine-grain grid search with k values around to your previous best value\n",
    "\n",
    "❓ What is the `best_score` and `best_k` you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0.7655062000041237\n"
     ]
    }
   ],
   "source": [
    "# Instanciate model\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "k_grid = {'n_neighbors' : np.arange(11,30,1)}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "grid = GridSearchCV(model, k_grid, n_jobs=-1,  cv = 5)\n",
    "\n",
    "# Fit data to Grid Search\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_params_[\"n_neighbors\"])\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = grid.best_params_[\"n_neighbors\"]\n",
    "best_score = grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧪 Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.5, pytest-6.2.1, py-1.9.0, pluggy-0.13.1 -- /Users/brunolajoie/.pyenv/versions/3.8.5/envs/lewagon502/bin/python3.8\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/brunolajoie/code/lewagon/data-solutions/05-ML/05-Model-Tuning/01-Workflow\n",
      "plugins: dash-1.18.1, anyio-2.0.2, pylint-0.17.0\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tests/test_knn.py::TestKnn::test_best_k PASSED                           [ 50%]\n",
      "tests/test_knn.py::TestKnn::test_best_score PASSED                       [100%]\n",
      "\n",
      "============================== 2 passed in 0.16s ===============================\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/knn.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed knn step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('knn',\n",
    "                         best_k=best_k,\n",
    "                         best_score=best_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ This problem is actually simple enough to perform a grid search manually.\n",
    "- Loop manually over all values of k from 1 to 50 and store the mean cv-scores of each model in a list.\n",
    "- Plot the score as a function of k to visualy find the best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for k in range(1,50):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    scores.append(cross_validate(knn, X_train_scaled, y_train, cv=5)[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓Can you guess what makes GridSearchCV a better option than such manual loop ?\n",
    " \n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "- Sklearn's `n_jobs=-1` allows you to paralellize search of each CPU\n",
    "- What if you had multiple hyper-parameters to co-optimize ?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNNRegressor suppports various _distance metrics_ via the hyper-parameter `p` [see docs](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "❓Use GridSearchCV to search for best `k` and `p` at the same time: Try all combinations for `k` = [1, 5, 10, 20, 50] and `p` = [1, 2, 3]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={'n_neighbors': [1, 5, 10, 20, 50], 'p': [1, 2, 3]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsRegressor()\n",
    "k_grid = {'n_neighbors' : [1, 5, 10, 20, 50],\n",
    "          'p': [1,2,3]}\n",
    "grid = GridSearchCV(model, k_grid,\n",
    "                     cv = 5, n_jobs=-1)\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ How many models did you trained overall?\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "Much more than 15. Think twice :)\n",
    "    <details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "75 models due to CV=5\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75 models due to CV=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What are the best parameters and the best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 10, 'p': 1}\n",
      "0.7978142226309175\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Search\n",
    "\n",
    "Now let's see if a Random Search can find a better combination with the same number of model fits?\n",
    "Use `RandomizedSearchCV` to\n",
    "- Randomly sample `k` from a uniform `randint(1,50)` distribition\n",
    "- Sample `p` from a list [1,2,3]\n",
    "- Use the correct number of `n_iter` and `cv` to fit the exact same number of models than in your previous GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7978142226309175\n",
      "{'n_neighbors': 10, 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "search_space = {'n_neighbors': randint(1, 50), 'p': [1, 2, 3]}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_distributions=search_space,\n",
    "                            n_jobs=-1,  cv=5, n_iter=15)\n",
    "\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 This is your final chance to fine-tune your model\n",
    "- Refine your RandomsearchCV if you wish\n",
    "- Choose your best model hyper-params and instantiate it\n",
    "- Re-fit it on the __entire__ train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=10, p=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = search.best_estimator_\n",
    "best_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Time has come to discover our model's performance on the **unseen** test set `X_test`. Compute the r2 score for the test set and save it as `r2_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7706993720504276"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_test = r2_score(y_test, best_model.predict(scaler.transform(X_test)))\n",
    "r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Would you consider the optimized model to generalize well?\n",
    "\n",
    "<details><summary>Answer</summary>\n",
    "\n",
    "Test score may decrease a bit with train set. Probably not more than 5%. This can be due to\n",
    "- An non-representative train/test split\n",
    "- A cross-val number too small leading to overfitting the model-tuning phase. The more you cross-validated, the more robust your findings will generalize - but you can't increase cv too much if your dataset is too small as you won't keep enough observations in each fold to be representative.\n",
    "- Our dataset is very small and our hyperparameter optimization is thus extremely dependent (and overfitting) on our train/test split. Always make sure your dataset is much bigger than the total number of hyperparameter combinations you are trying out!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧪 Test your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.5, pytest-6.2.1, py-1.9.0, pluggy-0.13.1 -- /Users/brunolajoie/.pyenv/versions/3.8.5/envs/lewagon502/bin/python3.8\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/brunolajoie/code/lewagon/data-solutions/05-ML/05-Model-Tuning/01-Workflow\n",
      "plugins: dash-1.18.1, anyio-2.0.2, pylint-0.17.0\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tests/test_r2.py::TestR2::test_r2 PASSED                                 [100%]\n",
      "\n",
      "============================== 1 passed in 0.12s ===============================\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/r2.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed r2 step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('r2', \n",
    "                         r2_test=r2_test)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Congratulation. Please push the exercise once completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
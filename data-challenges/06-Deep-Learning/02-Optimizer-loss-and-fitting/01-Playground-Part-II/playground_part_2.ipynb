{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 2 - Playground Part II\n",
    "\n",
    "**Goal**:\n",
    "Get a better understanding of Neural Network hyperparameters\n",
    "\n",
    "<hr>\n",
    "\n",
    "Open the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.06711&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=false&regularizationRate_hide=false) again to learn about Neural Network. \n",
    "\n",
    "Keep in mind that as the algorithm is stochastic, the results might differ from one run to the other. For this reason, do not hesitate to rerun the algorithms multiple times to be sure of your deductions and reasonings.\n",
    "\n",
    "Let's explore the different items we have seen during the lecture:\n",
    "1. Batch Size\n",
    "2. Regularization\n",
    "3. Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The batch size\n",
    "\n",
    "‚ùì **Question** ‚ùì Select the `circle dataset` (classification). \n",
    "\n",
    "Build a model with: \n",
    "* one hidden layer with 3 neurons,\n",
    "* a `learning rate` of 0.03, \n",
    "* and the `tanh` activation function\n",
    "\n",
    "Do not add any noise (=0).\n",
    "\n",
    "Now, select a batch size of 30 and look at the convergence of the algorithm. Does it seem slow or fast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now, run this neural network on the same data but with a batch-size of 1? Make sure to run at least 150 epochs. What do you notice on the train and test loss? What is the reason of this instability? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now, you can see the effect of the `batch_size` by reading the values of the train loss and test loss: pause the iterations and run it step by step (iteration per iteration) thanks to the `\"Step\"` button (at the right side of the play/stop button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì To observe once again the **lack of generalization**,  select the `\"eXclusive OR\"(XOR)` dataset, with a noise of 50. Add a second hidden layer with again 8 neurons. Try to fit your model: it should overfit!\n",
    "\n",
    "With a smaller batch size, your model will end up overfitting faster.\n",
    "\n",
    "Let's keep `batch size = 1` though for the next question and try to understand how to prevent this overfitting phenomeon using the strategy of `regularization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Can we **regularize** our network to avoid overfitting? Keep batch size to 1, and add a `L2-regularization`. Increase the power of this L2-regularization until it smoothes out the predictions! Notice how the test loss doesn't increase anymore with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now:\n",
    "\n",
    "* select the `spiral dataset`,\n",
    "* remove regularization, \n",
    "* and increase the `ratio of training to test data` to 80%. \n",
    "\n",
    "For the neural network, use:\n",
    "* 3 hidden layers with 8 neurons on the first layer, \n",
    "* 7 on the second,\n",
    "* and 6 on the third. \n",
    "\n",
    "Run the algorithm with a batch size of 30. \n",
    "\n",
    "Make sure to run it for at least 1500 epochs. \n",
    "\n",
    "Then, compare it to the same run but with a batch size of 1. \n",
    "\n",
    "You can check what happens on the train loss and test loss step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The learning rate\n",
    "\n",
    "Go back to the `circle dataset`:\n",
    "* with no noise,\n",
    "* and a *ratio of training to test data* of 50%.\n",
    "* Use a batch size of 20. \n",
    "\n",
    "Use a neural network with:\n",
    "* one layer of 5 neurons,\n",
    "* no regularization, \n",
    "* and the tanh activation function)\n",
    "\n",
    "‚ùì **Question** ‚ùì For each learning rate (from 0.0001 to 10), run the algorithm during 1000 epochs and report the values of the test loss in the list below. Then, plot the test loss with respect to the learning rates. \n",
    "\n",
    "‚ö†Ô∏è Warning ‚ö†Ô∏è When you change the learning rate, make sure to reinitialize the neural network (circular arrow, left to the play/pause button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "test_loss = [### YOUR LOSS VALUES]\n",
    "\n",
    "\n",
    "plt.plot(np.log(learning_rates), test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning ‚ö†Ô∏è It is important to understand that, even though small and high learning rates both have high test loss, this is absolutely not for the same reasons !. \n",
    "- on one hand, small learning rates help a Neural Net converges similarly to moderate learning rates, but way slower. \n",
    "- on the other hand, large learning rates make the algorithm diverge.\n",
    "\n",
    "\n",
    "To convince yourself of the lack of convergence when using high learning rates, select a learning rate equal to 10 and run it multiple times, each time with 400 epochs: your should see the variability of values, which corresponds to the fact that the algorithms converge to different local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations! Don't forget to commit and push your notebook** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
